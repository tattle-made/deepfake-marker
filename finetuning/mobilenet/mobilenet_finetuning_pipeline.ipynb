{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e0f245-aea0-4dee-9911-875224b7af06",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets==3.3.1\n",
    "!pip install pandas==2.2.3\n",
    "!pip install numpy==1.26.4\n",
    "!pip install torch==2.5.1\n",
    "!pip install torchaudio==2.5.1\n",
    "!pip install librosa==0.10.2.post1\n",
    "!pip install tqdm==4.67.1\n",
    "!pip install matplotlib==3.7.5\n",
    "!pip install tensorflow==2.18.0\n",
    "!pip install keras==3.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747a0863-2861-4c1c-aa36-51776c775dea",
   "metadata": {},
   "source": [
    "Documentation to get hf token : https://huggingface.co/docs/hub/en/security-tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ca2e7e-9637-4e7a-89e9-4f8cf683339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"nuriachandra/Deepfake-Eval-2024\", token=\"your_hf_token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ef472c-199d-4a43-97de-e5089651645a",
   "metadata": {},
   "source": [
    "Access metadata from here : https://huggingface.co/datasets/nuriachandra/Deepfake-Eval-2024/resolve/main/audio-metadata-publish.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4101e2-756b-448f-bdac-4ab4809d7eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./audio_metadata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dc38e5-5427-4e40-90c3-d3bead9032a9",
   "metadata": {},
   "source": [
    "### Extracting spectogram images for Deepfake-Eval-2024 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eb272f-c712-43e2-89e1-a90c42eac804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "\n",
    "output_dir = \"./mel_spectrograms\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "metadata_df = pd.read_csv(\"./audio_metadata.csv\")\n",
    "metadata_df = metadata_df.set_index('Filename')\n",
    "\n",
    "def waveform_to_mel_image(waveform, sample_rate, out_path):\n",
    "    waveform = torch.tensor(waveform, dtype=torch.float32)\n",
    "    if waveform.dim() == 1:\n",
    "        waveform = waveform.unsqueeze(0)\n",
    "\n",
    "    mel_transform = T.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=1024,\n",
    "        hop_length=512,\n",
    "        n_mels=64\n",
    "    )\n",
    "    mel_spec = mel_transform(waveform)\n",
    "    db_transform = T.AmplitudeToDB(top_db=80)\n",
    "    mel_db = db_transform(mel_spec)\n",
    "    mel_np = mel_db.squeeze(0).numpy()\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(mel_np, origin='lower', aspect='auto', cmap='inferno')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.savefig(out_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "dataset_split = dataset['train']\n",
    "\n",
    "for index in tqdm(range(len(dataset_split))):\n",
    "    try:\n",
    "        audio_data = dataset_split[index]['audio']\n",
    "        file_name = os.path.basename(audio_data['path'])\n",
    "        waveform, sample_rate = librosa.load(audio_data['path'], sr=None, mono=True)\n",
    "\n",
    "        out_path = os.path.join(output_dir, f\"{file_name.replace('.mp3', '')}.png\")\n",
    "\n",
    "        waveform_to_mel_image(waveform, sample_rate, out_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to convert sample at index {index}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1238ea35-ee10-4398-b4bb-72800e050c5b",
   "metadata": {},
   "source": [
    "### Data Loading function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73f0835-b5cf-4554-bcf9-77cc976fea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import os\n",
    "\n",
    "def load_images(path, label, df, split):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    valid_files = df[(df['Ground Truth'] == label) & (df['Finetuning Set'] == split)]['Filename']\n",
    "    valid_basenames = set(name.split('.')[0] for name in valid_files)\n",
    "\n",
    "    print(f\"Looking for {len(valid_basenames)} matching .png images corresponding to .wav/.mp3 entries.\")\n",
    "\n",
    "    i = 0\n",
    "    matched_files = []\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "        file_basename = file.split('.')[0] \n",
    "        if file_basename in valid_basenames:\n",
    "            matched_files.append(file)\n",
    "            try:\n",
    "                img_array = image.img_to_array(image.load_img(os.path.join(path, file), target_size=(224, 224, 3)))\n",
    "                images.append(img_array)\n",
    "                if label=='Real':\n",
    "                    labels.append(0)\n",
    "                else:\n",
    "                    labels.append(1)\n",
    "\n",
    "                i += 1\n",
    "                if i % 50 == 0:\n",
    "                    print('Loaded', i, 'images')\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {file}: {e}\")\n",
    "\n",
    "    print(f'\\nTotal {label} {split.lower()} images loaded = {i}')\n",
    "    print(f\"Example matched: {matched_files[:5]}\")\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ec55e5-ba0b-4a83-8648-ccb643b7d4d7",
   "metadata": {},
   "source": [
    "### Defining training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b13de99-5b76-4b08-b89a-45ba5fcb28bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f8480a-45ae-426c-bb07-487363ca65a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_images('./mel_spectrograms', 'Real',df,'Train')\n",
    "\n",
    "x += images\n",
    "y += labels\n",
    "\n",
    "print('Image shape:',x[0].shape)\n",
    "print('Image label (REAL):',y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002beb59-db44-4aee-a5ad-425ef646e3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_images('./mel_spectrograms', 'Fake',df,'Train')\n",
    "    \n",
    "x += images\n",
    "y += labels\n",
    "\n",
    "print('Image shape:',x[0].shape)\n",
    "print('Image label (FAKE):',y[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5520b49c-60d9-44d1-81bd-5ce093fa24d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = []\n",
    "y_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc577b4-d67b-4cf3-ba21-426aa077b289",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_images('./mel_spectrograms', 'Real',df,'Test')\n",
    "\n",
    "x_test += images\n",
    "y_test += labels\n",
    "\n",
    "print('Image shape:',x[0].shape)\n",
    "print('Image label (REAL):',y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b546cfb-5b2c-42de-a32d-f405242c851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_images('./mel_spectrograms', 'Fake',df,'Test')\n",
    "\n",
    "x_test += images\n",
    "y_test += labels\n",
    "\n",
    "print('Image shape:',x[0].shape)\n",
    "print('Image label (REAL):',y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f75dce9-ba04-4b8b-82de-e352f7cc3d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "x_train_norm = np.array(x) / 255\n",
    "x_test_norm = np.array(x_test) / 255\n",
    "\n",
    "y_train_encoded = to_categorical(y)\n",
    "y_test_encoded = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693abd93-49f1-4812-b087-970381fc614c",
   "metadata": {},
   "source": [
    "### Loading and Defining Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e2ced5-3ab7-467c-8525-42c567ea1b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Flatten, Dense\n",
    "import tensorflow.keras as K\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',     \n",
    "    patience=5,               \n",
    "    restore_best_weights=True  \n",
    ")\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "base_model = MobileNet(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11f44ad-f330-46b5-b577-f70b56c76616",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=base_model.input, outputs=x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befb25f3-4b18-4ee4-8480-2b95d2e933a4",
   "metadata": {},
   "source": [
    "Freezing weights for deeper layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d049613c-c51e-4639-a9a3-229ad9e9256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8989fa18-b270-42fb-a515-99deaee95f9f",
   "metadata": {},
   "source": [
    "### Training and Testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11320c47-dffa-49ad-b5a5-979182fb32a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "hist = model.fit(\n",
    "    x_train_norm, y_train_encoded, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_data=(x_test_norm, y_test_encoded),\n",
    "    callbacks=[early_stopping] \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
